---
title: "Course Project Writeup"
output:
  html_document:
    keep_md: yes
    toc: yes
---

### Introduction

The purpose of this project is to analyse a dataset from the [*Quantified Self Movement*](http://quantifiedself.com/),
a group of enthusiasts who regularly take measurements about themselves to improve 
their health, to find patterns in their behavior, or because they are tech geeks. 
Measures may come from sensor in devices that can be worn, like accelerometers
in the belt or bracelet. In this dataset we analyse measurements related to
weight lifting exercises performed by six subjects, in order to predict the manner
in which they did them. More information available on http://groupware.les.inf.puc-rio.br/har.

This is an assignment for the 3rd week of the Practical Machine Learning course by the Johns Hopkins University on [Coursera](http://www.coursera.org/).

### Raw data source

The raw data consist of training and test datasets for the study.

The training dataset is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test dataset is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Loading the data
Let's load the data paying attention at the NA values; 
the first 6 variables identify user and time of the observation and thus
are not observation values; the last variable is the outcome that has to be
predicted and will be treated as a factor; the variables used as
predictors will be selected from the ones with less than 5% of NAs.
```{r cache=TRUE}
trainDS<-read.csv('pml-training.csv',header=TRUE,sep=',',quote='"',na.strings=c('#DIV/0!','NA'),row.names=1)
testDS <-read.csv('pml-testing.csv', header=TRUE,sep=',',quote='"',na.strings=c('#DIV/0!','NA'),row.names=1)
dim(trainDS)    # focus only on trainDS now
th <- round(dim(trainDS)[1] * .05) # threshold for NA: 5% of the rows
completeColumns   <- sapply(trainDS[,7:158], function(x) sum(is.na(x)) < th)
completeVariables <- names(completeColumns[completeColumns==TRUE]); completeVariables
trainSubset <- trainDS[c(completeVariables, "classe")] # actual training subset
trainSubset$classe <- factor(trainSubset$classe)
```

`r length(completeVariables)` variables have less than `r th` NAs (5% of `r dim(trainSubset)[1]`)
and will be considered possible predictors.

### Exploratory Analysis

Let's now get a subset of the data to perform a quick exploratory analysis, to get
an idea of the distribution of the outcome variable.
```{r message=FALSE, cache=TRUE, fig.align='center', fig.height=8}
library(dplyr); explDS <- trainDS %>% select(user_name, time=cvtd_timestamp, classe) %>%
    group_by(user_name, time, classe) %>% summarise(num_obs=length(classe))
library(ggplot2); ggplot(explDS, aes(x=time, y=num_obs, fill=classe)) + facet_grid(user_name ~ .) +
    geom_bar(stat='identity',position='dodge') + theme(axis.text.x=element_text(angle=90,vjust=0.5))
```

The outcome variable does not seem to follow a linear trend, therefore linear
predictors are unlikely to work well: we'll then try directly with a random forest approach.

**NOTE:** It might be unclear if the variable *user_name* is to be considered a predictor. 
The assigment explicitly asks to *predict activity quality from activity monitors*,
therefore here it is excluded.

### Partitioning and Training

Let's now create the actual training and test dataset out of the *trainSubset*,
splitting it into the usual 60% - 40% partitions.
These will be used for the in-sampling assessment and validation:
```{r message=FALSE}
rm(trainDS, explDS) # save some memory
set.seed (123456)   # for reproducibility
library(caret); indexTraining <- createDataPartition(trainSubset$classe, p=0.6, list=TRUE)[[1]]
trainTraining <- trainSubset[indexTraining,]; trainTesting <- trainSubset[-indexTraining,]
```

The *training* with random forests method is computationally intensive and can 
be speeded up with parallel processing. Now check the accuracy of the model:
```{r message=FALSE, cache=TRUE}
library(doParallel); cluster<-makeCluster(detectCores()-1); registerDoParallel(cluster)
model=train(classe~., data=trainTraining, method='rf',trControl=trainControl('oob',seeds=list(123)),ntree=200)
stopCluster(cluster); registerDoSEQ()  # switch back to single-core processing
model$finalModel
model$results[model$results$mtry == model$finalModel$mtry,]
```

The model accuracy is `r model$results$Accuracy[model$results$mtry==model$finalModel$mtry]`,
therefore the in-sample error rate is 
1-`r model$results$Accuracy[model$results$mtry==model$finalModel$mtry]` = `r 1-model$results$Accuracy[model$results$mtry==model$finalModel$mtry]`. The out-of-sample error rate should be close to this value (usually
a bit bigger): if not, the model could be overfitting the training dataset.

### Cross-Validation

And now, cross-validate with the testing subset:
```{r message=FALSE}
Q <- confusionMatrix(predict(model,trainTesting),trainTesting$classe)
Q; Q$overall["Accuracy"]
```

The out-of-sample error rate is 1-`r Q$overall["Accuracy"]`=`r 1-Q$overall["Accuracy"]`, therefore
the model provides a pretty good estimate. 

### Prediction

On this basis, the prediction on the test dataset is:
```{r message=FALSE}
answers <- predict(model,testDS); answers
```

Finally, here is the code to generate the files for the project submission:
```{r message=FALSE}
pml_write_files = function(x) {
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files (answers)
```
